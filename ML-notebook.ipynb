{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32f1a4d",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Introduction to Machine Learning\n",
    "- Definition and Overview\n",
    "  - What is Machine Learning?\n",
    "  - History and Evolution\n",
    "  - Key Concepts and Terminology\n",
    "- Types of Machine Learning\n",
    "  - Supervised Learning\n",
    "    - Definition and Examples\n",
    "    - Use Cases\n",
    "    - Algorithms\n",
    "  - Unsupervised Learning\n",
    "    - Definition and Examples\n",
    "    - Use Cases\n",
    "    - Algorithms\n",
    "  - Semi-supervised Learning\n",
    "    - Definition and Examples\n",
    "    - Use Cases\n",
    "    - Algorithms\n",
    "  - Reinforcement Learning\n",
    "    - Definition and Examples\n",
    "    - Use Cases\n",
    "    - Algorithms\n",
    "- Applications of Machine Learning\n",
    "  - Healthcare\n",
    "  - Finance\n",
    "  - Retail\n",
    "  - Autonomous Vehicles\n",
    "  - Natural Language Processing\n",
    "\n",
    "## Data Preprocessing\n",
    "- Data Cleaning\n",
    "  - Handling Missing Values\n",
    "    - Mean/Median Imputation\n",
    "    - Dropping Missing Values\n",
    "    - Filling with Forward/Backward Fill\n",
    "  - Handling Outliers\n",
    "    - Z-Score Method\n",
    "    - IQR Method\n",
    "    - Winsorization\n",
    "- Data Transformation\n",
    "  - Encoding Categorical Variables\n",
    "    - One-Hot Encoding\n",
    "    - Label Encoding\n",
    "    - Binary Encoding\n",
    "  - Feature Scaling\n",
    "    - Normalization (Min-Max Scaling)\n",
    "    - Standardization (Z-Score Scaling)\n",
    "  - Feature Engineering\n",
    "    - Creating New Features\n",
    "    - Polynomial Features\n",
    "    - Interaction Features\n",
    "    - Log Transformations\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "- Descriptive Statistics\n",
    "  - Measures of Central Tendency (Mean, Median, Mode)\n",
    "  - Measures of Dispersion (Variance, Standard Deviation, Range)\n",
    "  - Skewness and Kurtosis\n",
    "- Data Visualization Techniques\n",
    "  - Histograms\n",
    "  - Box Plots\n",
    "  - Scatter Plots\n",
    "  - Pair Plots\n",
    "  - Heatmaps\n",
    "  - Violin Plots\n",
    "- Identifying Patterns and Relationships\n",
    "  - Correlation Analysis\n",
    "    - Pearson Correlation\n",
    "    - Spearman Correlation\n",
    "  - Trend Analysis\n",
    "  - Detecting Seasonality\n",
    "\n",
    "## Supervised Learning\n",
    "- Regression Algorithms\n",
    "  - Linear Regression\n",
    "    - Assumptions\n",
    "    - Model Building\n",
    "    - Evaluation Metrics (MSE, RMSE, R²)\n",
    "  - Polynomial Regression\n",
    "    - Polynomial Features\n",
    "    - Overfitting and Underfitting\n",
    "  - Ridge and Lasso Regression\n",
    "    - Regularization Techniques\n",
    "    - Hyperparameter Tuning (α)\n",
    "  - Support Vector Regression\n",
    "    - Kernel Trick\n",
    "    - Epsilon-Insensitive Loss\n",
    "- Classification Algorithms\n",
    "  - Logistic Regression\n",
    "    - Sigmoid Function\n",
    "    - Thresholding\n",
    "    - ROC Curve and AUC\n",
    "  - k-Nearest Neighbors (k-NN)\n",
    "    - Distance Metrics (Euclidean, Manhattan)\n",
    "    - Choosing k\n",
    "  - Support Vector Machines (SVM)\n",
    "    - Linear SVM\n",
    "    - Kernel SVM (RBF, Polynomial)\n",
    "    - Hyperparameters (C, Gamma)\n",
    "  - Decision Trees\n",
    "    - Splitting Criteria (Gini, Entropy)\n",
    "    - Pruning Techniques\n",
    "  - Random Forests\n",
    "    - Bagging\n",
    "    - Feature Importance\n",
    "  - Gradient Boosting\n",
    "    - Boosting Principle\n",
    "    - Variants (AdaBoost, Gradient Boosting, XGBoost, LightGBM)\n",
    "  - Neural Networks\n",
    "    - Perceptrons\n",
    "    - Multilayer Perceptrons (MLP)\n",
    "    - Activation Functions (ReLU, Sigmoid, Tanh)\n",
    "\n",
    "## Unsupervised Learning\n",
    "- Clustering Algorithms\n",
    "  - k-Means Clustering\n",
    "    - Choosing k (Elbow Method, Silhouette Score)\n",
    "    - Initial Centroid Selection\n",
    "  - Hierarchical Clustering\n",
    "    - Agglomerative vs. Divisive\n",
    "    - Dendrograms\n",
    "  - DBSCAN\n",
    "    - Density-Based Clustering\n",
    "    - Parameters (Epsilon, MinPts)\n",
    "- Dimensionality Reduction\n",
    "  - Principal Component Analysis (PCA)\n",
    "    - Eigenvalues and Eigenvectors\n",
    "    - Explained Variance Ratio\n",
    "  - t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    - Perplexity\n",
    "    - Use Cases\n",
    "  - Linear Discriminant Analysis (LDA)\n",
    "    - Maximizing Class Separability\n",
    "    - Comparison with PCA\n",
    "\n",
    "## Semi-Supervised Learning\n",
    "- Self-Training\n",
    "  - Pseudo-Labeling\n",
    "    - Confidence Thresholds\n",
    "    - Iterative Refinement\n",
    "- Co-Training\n",
    "  - View Selection\n",
    "    - Independent Feature Sets\n",
    "- Graph-Based Semi-Supervised Learning\n",
    "  - Label Propagation\n",
    "  - Graph Convolutional Networks\n",
    "\n",
    "## Reinforcement Learning\n",
    "- Introduction to Reinforcement Learning\n",
    "  - Basics of RL\n",
    "  - Key Concepts: Agent, Environment, State, Action, Reward\n",
    "- Markov Decision Processes (MDP)\n",
    "  - States and Actions\n",
    "  - Transition Probabilities\n",
    "  - Rewards\n",
    "- Q-Learning\n",
    "  - Q-Table\n",
    "  - Bellman Equation\n",
    "  - Exploration vs. Exploitation\n",
    "- Deep Q-Networks (DQN)\n",
    "  - Neural Network Architecture\n",
    "  - Experience Replay\n",
    "  - Target Networks\n",
    "- Policy Gradient Methods\n",
    "  - REINFORCE Algorithm\n",
    "  - Actor-Critic Methods\n",
    "\n",
    "## Model Evaluation and Selection\n",
    "- Train/Test Split\n",
    "  - Holdout Validation\n",
    "  - Stratified Sampling\n",
    "- Cross-Validation\n",
    "  - k-Fold Cross-Validation\n",
    "    - Advantages and Disadvantages\n",
    "  - Leave-One-Out Cross-Validation\n",
    "    - Use Cases\n",
    "- Evaluation Metrics for Regression\n",
    "  - Mean Squared Error (MSE)\n",
    "  - Root Mean Squared Error (RMSE)\n",
    "  - Mean Absolute Error (MAE)\n",
    "  - R² Score\n",
    "- Evaluation Metrics for Classification\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1 Score\n",
    "  - ROC Curve and AUC\n",
    "  - Confusion Matrix\n",
    "- Overfitting and Underfitting\n",
    "  - Bias-Variance Tradeoff\n",
    "  - Techniques to Mitigate Overfitting (Regularization, Pruning, Early Stopping)\n",
    "- Model Selection and Hyperparameter Tuning\n",
    "  - Grid Search\n",
    "  - Random Search\n",
    "  - Bayesian Optimization\n",
    "\n",
    "## Ensemble Learning\n",
    "- Bagging\n",
    "  - Bootstrap Aggregating\n",
    "  - Random Forests\n",
    "    - Out-of-Bag Error\n",
    "- Boosting\n",
    "  - AdaBoost\n",
    "    - Weight Updates\n",
    "  - Gradient Boosting\n",
    "    - Gradient Descent\n",
    "    - Learning Rate\n",
    "  - XGBoost\n",
    "    - Regularization Parameters\n",
    "  - LightGBM\n",
    "    - Leaf-wise Growth\n",
    "- Stacking\n",
    "  - Base Models\n",
    "  - Meta-Model\n",
    "- Voting Classifiers\n",
    "  - Hard Voting\n",
    "  - Soft Voting\n",
    "\n",
    "## Neural Networks and Deep Learning\n",
    "- Basics of Neural Networks\n",
    "  - Perceptrons\n",
    "  - Multilayer Perceptrons (MLP)\n",
    "    - Forward Propagation\n",
    "    - Backpropagation\n",
    "- Activation Functions\n",
    "  - Sigmoid\n",
    "  - Tanh\n",
    "  - ReLU\n",
    "  - Leaky ReLU\n",
    "- Training Neural Networks\n",
    "  - Gradient Descent\n",
    "    - Batch Gradient Descent\n",
    "    - Stochastic Gradient Descent\n",
    "    - Mini-Batch Gradient Descent\n",
    "  - Optimizers\n",
    "    - Adam\n",
    "    - RMSprop\n",
    "    - Adagrad\n",
    "- Convolutional Neural Networks (CNNs)\n",
    "  - Convolutional Layers\n",
    "  - Pooling Layers\n",
    "  - Fully Connected Layers\n",
    "  - Dropout Regularization\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "  - Basic RNN\n",
    "  - Long Short-Term Memory (LSTM) Networks\n",
    "  - Gated Recurrent Unit (GRU)\n",
    "- Autoencoders\n",
    "  - Basic Autoencoder\n",
    "  - Denoising Autoencoder\n",
    "  - Variational Autoencoder (VAE)\n",
    "- Generative Adversarial Networks (GANs)\n",
    "  - Generator and Discriminator\n",
    "  - Training GANs\n",
    "  - Applications (Image Generation, Style Transfer)\n",
    "\n",
    "## Natural Language Processing (NLP)\n",
    "- Text Preprocessing\n",
    "  - Tokenization\n",
    "    - Word Tokenization\n",
    "    - Sentence Tokenization\n",
    "  - Lemmatization\n",
    "  - Stemming\n",
    "  - Stop Words Removal\n",
    "  - N-grams\n",
    "- Bag-of-Words (BoW)\n",
    "  - Vector Representation\n",
    "  - Sparsity Issues\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "  - Calculation\n",
    "  - Applications\n",
    "- Word Embeddings\n",
    "  - Word2Vec\n",
    "    - CBOW and Skip-Gram\n",
    "  - GloVe\n",
    "  - FastText\n",
    "- Sequence Models\n",
    "  - Recurrent Neural Networks (RNN)\n",
    "  - Long Short-Term Memory (LSTM)\n",
    "  - Gated Recurrent Unit (GRU)\n",
    "- Attention Mechanisms and Transformers\n",
    "  - Attention Mechanisms\n",
    "    - Self-Attention\n",
    "    - Multi-Head Attention\n",
    "  - Transformers\n",
    "    - Encoder-Decoder Architecture\n",
    "    - BERT\n",
    "    - GPT\n",
    "- Retrieval-Augmented Generation (RAG)\n",
    "  - Overview\n",
    "  - Applications\n",
    "\n",
    "## Time Series Analysis\n",
    "- Time Series Decomposition\n",
    "  - Additive and Multiplicative Models\n",
    "  - Trend, Seasonality, and Residuals\n",
    "- ARIMA Models\n",
    "  - Autoregressive (AR)\n",
    "  - Integrated (I)\n",
    "  - Moving Average (MA)\n",
    "  - ARIMA Model Building\n",
    "- Exponential Smoothing\n",
    "  - Simple Exponential Smoothing\n",
    "  - Holt’s Linear Trend Model\n",
    "  - Holt-Winters Seasonal Model\n",
    "- Long Short-Term Memory (LSTM) for Time Series\n",
    "  - Sequence Prediction\n",
    "  - Handling Long Sequences\n",
    "\n",
    "## Anomaly Detection\n",
    "- Statistical Methods\n",
    "  - Z-Score\n",
    "  - IQR\n",
    "  - Moving Average\n",
    "- Machine Learning Methods\n",
    "  - Isolation Forest\n",
    "  - One-Class SVM\n",
    "  - k-Means Clustering\n",
    "- Deep Learning Methods\n",
    "  - Autoencoders\n",
    "  - Generative Adversarial Networks (GANs)\n",
    "\n",
    "## Model Deployment\n",
    "- Saving and Loading Models\n",
    "  - Using Pickle\n",
    "  - Using Joblib\n",
    "- Model Serving\n",
    "  - REST APIs (Flask, FastAPI)\n",
    "  - Web Services (Django, Flask)\n",
    "  - Cloud Services (AWS SageMaker, Google AI Platform)\n",
    "- Monitoring and Maintenance\n",
    "  - Model Performance Monitoring\n",
    "  - A/B Testing\n",
    "  - Model Retraining\n",
    "\n",
    "## Tools and Libraries\n",
    "- Python Libraries\n",
    "  - NumPy\n",
    "  - pandas\n",
    "  - scikit-learn\n",
    "  - TensorFlow\n",
    "  - Keras\n",
    "  - PyTorch\n",
    "  - SciPy\n",
    "- Data Visualization Libraries\n",
    "  - Matplotlib\n",
    "  - Seaborn\n",
    "  - Plotly\n",
    "  - Bokeh\n",
    "  - Altair\n",
    "- Tools for Model Deployment\n",
    "  - Flask\n",
    "  - Django\n",
    "  - FastAPI\n",
    "  - Docker\n",
    "  - Kubernetes\n",
    "\n",
    "## Ethical and Responsible AI\n",
    "- Bias and Fairness\n",
    "  - Identifying and Mitigating Bias\n",
    "  - Fairness Metrics\n",
    "- Explainability and Interpretability\n",
    "  - LIME (Local Interpretable Model-agnostic Explanations)\n",
    "  - SHAP (SHapley Additive exPlanations)\n",
    "  - Model-Specific Methods (e.g., Feature Importance in Trees)\n",
    "- Privacy and Security\n",
    "  - Differential Privacy\n",
    "  - Federated Learning\n",
    "  - Secure Multi-Party Computation\n",
    "- Ethical Considerations in AI\n",
    "  - Ethical Guidelines (e.g., IEEE, ACM)\n",
    "  - Responsible AI Practices\n",
    "  - Case Studies and Best Practices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076e6c0",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "## Introduction to Machine Learning\n",
    "- Definition and Overview\n",
    "  - What is Machine Learning?\n",
    "  - History and Evolution\n",
    "  - Key Concepts and Terminology\n",
    "- Types of Machine Learning\n",
    "  - Supervised Learning\n",
    "    - Definition and Examples\n",
    "    - Use Cases\n",
    "    - Algorithms (e.g., `LinearRegression` from `sklearn.linear_model`)\n",
    "  - Unsupervised Learning\n",
    "    - Definition and Examples\n",
    "    - Use Cases\n",
    "    - Algorithms (e.g., `KMeans` from `sklearn.cluster`)\n",
    "  - Semi-supervised Learning\n",
    "    - Definition and Examples\n",
    "    - Use Cases\n",
    "    - Algorithms (e.g., Self-Training from `sklearn.semi_supervised`)\n",
    "  - Reinforcement Learning\n",
    "    - Definition and Examples\n",
    "    - Use Cases\n",
    "    - Algorithms (e.g., Q-Learning, Deep Q-Networks with `tensorflow` or `pytorch`)\n",
    "- Applications of Machine Learning\n",
    "  - Healthcare\n",
    "  - Finance\n",
    "  - Retail\n",
    "  - Autonomous Vehicles\n",
    "  - Natural Language Processing\n",
    "\n",
    "## Data Preprocessing\n",
    "- Data Cleaning\n",
    "  - Handling Missing Values\n",
    "    - Mean/Median Imputation (e.g., `SimpleImputer` from `sklearn.impute`)\n",
    "    - Dropping Missing Values (e.g., `dropna()` from `pandas`)\n",
    "    - Filling with Forward/Backward Fill (e.g., `fillna(method='ffill')` from `pandas`)\n",
    "  - Handling Outliers\n",
    "    - Z-Score Method (e.g., `scipy.stats.zscore`)\n",
    "    - IQR Method (e.g., using `quantile` from `pandas`)\n",
    "    - Winsorization (e.g., `winsorize` from `scipy.stats.mstats`)\n",
    "- Data Transformation\n",
    "  - Encoding Categorical Variables\n",
    "    - One-Hot Encoding (e.g., `OneHotEncoder` from `sklearn.preprocessing`)\n",
    "    - Label Encoding (e.g., `LabelEncoder` from `sklearn.preprocessing`)\n",
    "    - Binary Encoding (e.g., `binary` from `category_encoders`)\n",
    "  - Feature Scaling\n",
    "    - Normalization (Min-Max Scaling) (e.g., `MinMaxScaler` from `sklearn.preprocessing`)\n",
    "    - Standardization (Z-Score Scaling) (e.g., `StandardScaler` from `sklearn.preprocessing`)\n",
    "  - Feature Engineering\n",
    "    - Creating New Features (e.g., using `pandas`)\n",
    "    - Polynomial Features (e.g., `PolynomialFeatures` from `sklearn.preprocessing`)\n",
    "    - Interaction Features (e.g., using `pandas` and custom functions)\n",
    "    - Log Transformations (e.g., `numpy.log`)\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "- Descriptive Statistics\n",
    "  - Measures of Central Tendency (Mean, Median, Mode) (e.g., `mean`, `median`, `mode` from `numpy` or `pandas`)\n",
    "  - Measures of Dispersion (Variance, Standard Deviation, Range) (e.g., `var`, `std` from `numpy` or `pandas`)\n",
    "  - Skewness and Kurtosis (e.g., `skew`, `kurtosis` from `scipy.stats`)\n",
    "- Data Visualization Techniques\n",
    "  - Histograms (e.g., `hist` from `matplotlib.pyplot` or `seaborn`)\n",
    "  - Box Plots (e.g., `boxplot` from `matplotlib.pyplot` or `seaborn`)\n",
    "  - Scatter Plots (e.g., `scatter` from `matplotlib.pyplot` or `seaborn`)\n",
    "  - Pair Plots (e.g., `pairplot` from `seaborn`)\n",
    "  - Heatmaps (e.g., `heatmap` from `seaborn`)\n",
    "  - Violin Plots (e.g., `violinplot` from `seaborn`)\n",
    "- Identifying Patterns and Relationships\n",
    "  - Correlation Analysis\n",
    "    - Pearson Correlation (e.g., `corr` from `pandas`)\n",
    "    - Spearman Correlation (e.g., `spearmanr` from `scipy.stats`)\n",
    "  - Trend Analysis (e.g., using `pandas` time series methods)\n",
    "  - Detecting Seasonality (e.g., using `statsmodels.tsa`)\n",
    "\n",
    "## Supervised Learning\n",
    "- Regression Algorithms\n",
    "  - Linear Regression\n",
    "    - Assumptions\n",
    "    - Model Building (e.g., `LinearRegression` from `sklearn.linear_model`)\n",
    "    - Evaluation Metrics (MSE, RMSE, R²) (e.g., `mean_squared_error`, `r2_score` from `sklearn.metrics`)\n",
    "  - Polynomial Regression\n",
    "    - Polynomial Features (e.g., `PolynomialFeatures` from `sklearn.preprocessing`)\n",
    "    - Overfitting and Underfitting\n",
    "  - Ridge and Lasso Regression\n",
    "    - Regularization Techniques (e.g., `Ridge`, `Lasso` from `sklearn.linear_model`)\n",
    "    - Hyperparameter Tuning (α) (e.g., `GridSearchCV` from `sklearn.model_selection`)\n",
    "  - Support Vector Regression\n",
    "    - Kernel Trick (e.g., `SVR` from `sklearn.svm`)\n",
    "    - Epsilon-Insensitive Loss\n",
    "- Classification Algorithms\n",
    "  - Logistic Regression\n",
    "    - Sigmoid Function\n",
    "    - Thresholding\n",
    "    - ROC Curve and AUC (e.g., `roc_curve`, `auc` from `sklearn.metrics`)\n",
    "  - k-Nearest Neighbors (k-NN)\n",
    "    - Distance Metrics (Euclidean, Manhattan) (e.g., `KNeighborsClassifier` from `sklearn.neighbors`)\n",
    "    - Choosing k\n",
    "  - Support Vector Machines (SVM)\n",
    "    - Linear SVM\n",
    "    - Kernel SVM (RBF, Polynomial) (e.g., `SVC` from `sklearn.svm`)\n",
    "    - Hyperparameters (C, Gamma)\n",
    "  - Decision Trees\n",
    "    - Splitting Criteria (Gini, Entropy) (e.g., `DecisionTreeClassifier` from `sklearn.tree`)\n",
    "    - Pruning Techniques\n",
    "  - Random Forests\n",
    "    - Bagging (e.g., `RandomForestClassifier` from `sklearn.ensemble`)\n",
    "    - Feature Importance\n",
    "  - Gradient Boosting\n",
    "    - Boosting Principle\n",
    "    - Variants (AdaBoost, Gradient Boosting, XGBoost, LightGBM) (e.g., `GradientBoostingClassifier`, `XGBClassifier`, `LGBMClassifier`)\n",
    "  - Neural Networks\n",
    "    - Perceptrons\n",
    "    - Multilayer Perceptrons (MLP) (e.g., `MLPClassifier` from `sklearn.neural_network`)\n",
    "    - Activation Functions (ReLU, Sigmoid, Tanh)\n",
    "\n",
    "## Unsupervised Learning\n",
    "- Clustering Algorithms\n",
    "  - k-Means Clustering\n",
    "    - Choosing k (Elbow Method, Silhouette Score)\n",
    "    - Initial Centroid Selection (e.g., `KMeans` from `sklearn.cluster`)\n",
    "  - Hierarchical Clustering\n",
    "    - Agglomerative vs. Divisive\n",
    "    - Dendrograms (e.g., `AgglomerativeClustering` from `sklearn.cluster`, `dendrogram` from `scipy.cluster.hierarchy`)\n",
    "  - DBSCAN\n",
    "    - Density-Based Clustering\n",
    "    - Parameters (Epsilon, MinPts) (e.g., `DBSCAN` from `sklearn.cluster`)\n",
    "- Dimensionality Reduction\n",
    "  - Principal Component Analysis (PCA)\n",
    "    - Eigenvalues and Eigenvectors\n",
    "    - Explained Variance Ratio (e.g., `PCA` from `sklearn.decomposition`)\n",
    "  - t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    - Perplexity\n",
    "    - Use Cases (e.g., `TSNE` from `sklearn.manifold`)\n",
    "  - Linear Discriminant Analysis (LDA)\n",
    "    - Maximizing Class Separability\n",
    "    - Comparison with PCA (e.g., `LinearDiscriminantAnalysis` from `sklearn.discriminant_analysis`)\n",
    "\n",
    "## Semi-Supervised Learning\n",
    "- Self-Training\n",
    "  - Pseudo-Labeling\n",
    "    - Confidence Thresholds\n",
    "    - Iterative Refinement\n",
    "- Co-Training\n",
    "  - View Selection\n",
    "    - Independent Feature Sets\n",
    "- Graph-Based Semi-Supervised Learning\n",
    "  - Label Propagation (e.g., `LabelPropagation` from `sklearn.semi_supervised`)\n",
    "  - Graph Convolutional Networks\n",
    "\n",
    "## Reinforcement Learning\n",
    "- Introduction to Reinforcement Learning\n",
    "  - Basics of RL\n",
    "  - Key Concepts: Agent, Environment, State, Action, Reward\n",
    "- Markov Decision Processes (MDP)\n",
    "  - States and Actions\n",
    "  - Transition Probabilities\n",
    "  - Rewards\n",
    "- Q-Learning\n",
    "  - Q-Table\n",
    "  - Bellman Equation\n",
    "  - Exploration vs. Exploitation\n",
    "- Deep Q-Networks (DQN)\n",
    "  - Neural Network Architecture (e.g., using `tensorflow` or `pytorch`)\n",
    "  - Experience Replay\n",
    "  - Target Networks\n",
    "- Policy Gradient Methods\n",
    "  - REINFORCE Algorithm\n",
    "  - Actor-Critic Methods\n",
    "  \n",
    "## Model Evaluation and Selection\n",
    "- Train/Test Split\n",
    "  - Holdout Validation\n",
    "  - Stratified Sampling\n",
    "- Cross-Validation\n",
    "  - k-Fold Cross-Validation\n",
    "    - Advantages and Disadvantages\n",
    "  - Leave-One-Out Cross-Validation\n",
    "    - Use Cases\n",
    "- Evaluation Metrics for Regression\n",
    "  - Mean Squared Error (MSE)\n",
    "  - Root Mean Squared Error (RMSE)\n",
    "  - Mean Absolute Error (MAE)\n",
    "  - R² Score\n",
    "- Evaluation Metrics for Classification\n",
    "  - Accuracy\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1 Score\n",
    "  - ROC Curve and AUC\n",
    "  - Confusion Matrix\n",
    "- Overfitting and Underfitting\n",
    "  - Bias-Variance Tradeoff\n",
    "  - Techniques to Mitigate Overfitting (Regularization, Pruning, Early Stopping)\n",
    "- Model Selection and Hyperparameter Tuning\n",
    "  - Grid Search\n",
    "  - Random Search\n",
    "  - Bayesian Optimization\n",
    "  \n",
    "## Ensemble Learning\n",
    "- Bagging\n",
    "  - Bootstrap Aggregating\n",
    "  - Random Forests\n",
    "    - Out-of-Bag Error\n",
    "- Boosting\n",
    "  - AdaBoost\n",
    "    - Weight Updates\n",
    "  - Gradient Boosting\n",
    "    - Gradient Descent\n",
    "    - Learning Rate\n",
    "  - XGBoost\n",
    "    - Regularization Parameters\n",
    "  - LightGBM\n",
    "    - Leaf-wise Growth\n",
    "- Stacking\n",
    "  - Base Models\n",
    "  - Meta-Model\n",
    "- Voting Classifiers\n",
    "  - Hard Voting\n",
    "  - Soft Voting\n",
    "  \n",
    "## Neural Networks and Deep Learning\n",
    "- Basics of Neural Networks\n",
    "  - Perceptrons\n",
    "  - Multilayer Perceptrons (MLP)\n",
    "    - Forward Propagation\n",
    "    - Backpropagation\n",
    "- Activation Functions\n",
    "  - Sigmoid\n",
    "  - Tanh\n",
    "  - ReLU\n",
    "  - Leaky ReLU\n",
    "- Training Neural Networks\n",
    "  - Gradient Descent\n",
    "    - Batch Gradient Descent\n",
    "    - Stochastic Gradient Descent\n",
    "    - Mini-Batch Gradient Descent\n",
    "  - Optimizers\n",
    "    - Adam\n",
    "    - RMSprop\n",
    "    - Adagrad\n",
    "- Convolutional Neural Networks (CNNs)\n",
    "  - Convolutional Layers\n",
    "  - Pooling Layers\n",
    "  - Fully Connected Layers\n",
    "  - Dropout Regularization\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "  - Basic RNN\n",
    "  - Long Short-Term Memory (LSTM) Networks\n",
    "  - Gated Recurrent Unit (GRU)\n",
    "- Autoencoders\n",
    "  - Basic Autoencoder\n",
    "  - Denoising Autoencoder\n",
    "  - Variational Autoencoder (VAE)\n",
    "- Generative Adversarial Networks (GANs)\n",
    "  - Generator and Discriminator\n",
    "  - Training GANs\n",
    "  - Applications (Image Generation, Style Transfer)\n",
    "\n",
    "## Natural Language Processing (NLP)\n",
    "- Text Preprocessing\n",
    "  - Tokenization (e.g., `word_tokenize` from `nltk`, `Tokenizer` from `keras.preprocessing.text`)\n",
    "    - Word Tokenization\n",
    "    - Sentence Tokenization\n",
    "  - Stop Words Removal (e.g., `stopwords` from `nltk.corpus`)\n",
    "  - Stemming (e.g., `PorterStemmer` from `nltk.stem`)\n",
    "  - Lemmatization (e.g., `WordNetLemmatizer` from `nltk.stem`)\n",
    "  - N-grams\n",
    "- Feature Extraction\n",
    "  - Bag-of-Words (BoW)\n",
    "    - Count Vectorization (e.g., `CountVectorizer` from `sklearn.feature_extraction.text`)\n",
    "      - Vector Representation\n",
    "      - Sparsity Issues\n",
    "  - Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    - TF-IDF Vectorization (e.g., `TfidfVectorizer` from `sklearn.feature_extraction.text`)\n",
    "      - Calculation\n",
    "      - Applications\n",
    "  - Word Embeddings\n",
    "    - Word2Vec (e.g., `Word2Vec` from `gensim.models`)\n",
    "        - CBOW and Skip-Gram\n",
    "    - GloVe (Global Vectors for Word Representation)\n",
    "    - FastText\n",
    "- Text Classification\n",
    "  - Naive Bayes Classifier\n",
    "    - Multinomial Naive Bayes (e.g., `MultinomialNB` from `sklearn.naive_bayes`)\n",
    "  - Support Vector Machines (SVM)\n",
    "    - Linear SVM for Text Classification (e.g., `LinearSVC` from `sklearn.svm`)\n",
    "  - Recurrent Neural Networks (RNN)\n",
    "    - Long Short-Term Memory Networks (LSTM) (e.g., `LSTM` from `keras.layers`)\n",
    "    - Gated Recurrent Units (GRU)\n",
    "  - Transformer Models\n",
    "    - Encoder-Decoder Architecture\n",
    "    - BERT (e.g., `transformers.BertModel` from `transformers` library)\n",
    "    - GPT (e.g., `transformers.GPT2Model` from `transformers` library)\n",
    "- Text Generation\n",
    "  - Recurrent Neural Networks (RNN)\n",
    "    - Character-Level RNNs\n",
    "  - Transformer Models\n",
    "    - GPT-3, GPT-4 (e.g., `OpenAI API`)\n",
    "  - Retrieval-Augmented Generation (RAG)\n",
    "    - Combining Retrieval and Generation (e.g., `transformers.RagTokenizer`, `transformers.RagModel` from `transformers` library)\n",
    "    \n",
    "## Time Series Analysis\n",
    "- Time Series Decomposition\n",
    "  - Additive and Multiplicative Models\n",
    "  - Trend, Seasonality, and Residuals\n",
    "- ARIMA Models\n",
    "  - Autoregressive (AR)\n",
    "  - Integrated (I)\n",
    "  - Moving Average (MA)\n",
    "  - ARIMA Model Building\n",
    "- Exponential Smoothing\n",
    "  - Simple Exponential Smoothing\n",
    "  - Holt’s Linear Trend Model\n",
    "  - Holt-Winters Seasonal Model\n",
    "- Long Short-Term Memory (LSTM) for Time Series\n",
    "  - Sequence Prediction\n",
    "  - Handling Long Sequences\n",
    "\n",
    "## Anomaly Detection\n",
    "- Techniques and Algorithms\n",
    "  - Statistical Methods\n",
    "    - Z-Score (e.g., `scipy.stats.zscore`)\n",
    "    - Grubbs' Test\n",
    "  - Proximity-Based Methods\n",
    "    - k-Nearest Neighbors (k-NN) (e.g., `LocalOutlierFactor` from `sklearn.neighbors`)\n",
    "    - DBSCAN (e.g., `DBSCAN` from `sklearn.cluster`)\n",
    "  - Clustering-Based Methods\n",
    "    - k-Means Clustering (e.g., `KMeans` from `sklearn.cluster`)\n",
    "    - Isolation Forest (e.g., `IsolationForest` from `sklearn.ensemble`)\n",
    "  - Machine Learning-Based Methods\n",
    "    - One-Class SVM (e.g., `OneClassSVM` from `sklearn.svm`)\n",
    "    - Autoencoders (e.g., using `tensorflow.keras` or `pytorch`)\n",
    "\n",
    "## Model Deployment\n",
    "- Saving and Loading Models\n",
    "  - Using Pickle (e.g., `pickle.dump`, `pickle.load`)\n",
    "  - Using Joblib (e.g., `joblib.dump`, `joblib.load`)\n",
    "- Model Serving\n",
    "  - REST APIs (Flask, FastAPI)\n",
    "    - Building API Endpoints (e.g., `flask.Flask`, `fastapi.FastAPI`)\n",
    "    - Handling Requests and Responses\n",
    "  - Web Services (Django, Flask)\n",
    "    - Integrating Machine Learning Models\n",
    "    - Handling User Inputs (e.g., `django.http`, `django.views`)\n",
    "  - Cloud Services (AWS SageMaker, Google AI Platform)\n",
    "    - Deploying Models\n",
    "    - Monitoring and Scaling\n",
    "- Monitoring and Maintenance\n",
    "  - Model Performance Monitoring\n",
    "    - Tracking Metrics Over Time\n",
    "    - Setting Alerts for Degradation\n",
    "  - A/B Testing\n",
    "    - Designing Experiments\n",
    "    - Analyzing Results\n",
    "  - Model Retraining\n",
    "    - Triggering Retraining\n",
    "    - Automating Pipelines\n",
    "\n",
    "## Tools and Libraries\n",
    "- Python Libraries\n",
    "  - NumPy (e.g., `numpy.array`, `numpy.linalg`)\n",
    "  - pandas (e.g., `pandas.DataFrame`, `pandas.Series`)\n",
    "  - scikit-learn (e.g., `sklearn.preprocessing`, `sklearn.model_selection`)\n",
    "  - TensorFlow (e.g., `tensorflow.keras`, `tensorflow.data`)\n",
    "  - Keras (e.g., `keras.models`, `keras.layers`)\n",
    "  - PyTorch (e.g., `torch.Tensor`, `torch.nn`)\n",
    "  - SciPy (e.g., `scipy.stats`, `scipy.optimize`)\n",
    "- Data Visualization Libraries\n",
    "  - Matplotlib (e.g., `matplotlib.pyplot.plot`, `matplotlib.pyplot.show`)\n",
    "  - Seaborn (e.g., `seaborn.scatterplot`, `seaborn.heatmap`)\n",
    "  - Plotly (e.g., `plotly.graph_objs`, `plotly.express`)\n",
    "  - Bokeh (e.g., `bokeh.plotting.figure`, `bokeh.io.show`)\n",
    "  - Altair (e.g., `alt.Chart`, `alt.data_transformers`)\n",
    "- Tools for Model Deployment\n",
    "  - Flask (e.g., `flask.Flask`, `flask.request`)\n",
    "  - Django (e.g., `django.http`, `django.views`)\n",
    "  - FastAPI (e.g., `fastapi.FastAPI`, `fastapi.Request`)\n",
    "  - Docker (e.g., Dockerfiles, `docker-compose.yml`)\n",
    "  - Kubernetes (e.g., Pods, Deployments, Services)\n",
    "\n",
    "## Ethical and Responsible AI\n",
    "- Bias and Fairness\n",
    "  - Identifying and Mitigating Bias\n",
    "    - Data Bias Detection (e.g., `sklearn.metrics` fairness metrics)\n",
    "    - Algorithmic Fairness (e.g., Fairlearn toolkit)\n",
    "  - Fairness Metrics\n",
    "    - Demographic Parity\n",
    "    - Equalized Odds\n",
    "- Explainability and Interpretability\n",
    "  - LIME (Local Interpretable Model-agnostic Explanations)\n",
    "    - Using LIME (e.g., `lime.lime_tabular`)\n",
    "  - SHAP (SHapley Additive exPlanations)\n",
    "    - Using SHAP (e.g., `shap.TreeExplainer`, `shap.KernelExplainer`)\n",
    "  - Model-Specific Methods\n",
    "    - Feature Importance in Trees (e.g., `feature_importances_` in `sklearn.ensemble` models)\n",
    "- Privacy and Security\n",
    "  - Differential Privacy\n",
    "    - Adding Noise to Data\n",
    "    - Privacy-Preserving Mechanisms\n",
    "  - Federated Learning\n",
    "    - Training Across Multiple Devices\n",
    "    - Aggregating Results Securely\n",
    "  - Secure Multi-Party Computation\n",
    "    - Techniques and Protocols\n",
    "- Ethical Considerations in AI\n",
    "  - Ethical Guidelines (e.g., IEEE, ACM)\n",
    "  - Responsible AI Practices\n",
    "    - Transparency\n",
    "    - Accountability\n",
    "  - Case Studies and Best Practices\n",
    "    - Real-World Examples\n",
    "    - Lessons Learned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c354bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3b1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b99430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071ced99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3df884",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c39c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way. just different based on directory thing?\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d880a",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbddc23",
   "metadata": {},
   "source": [
    "resources:\n",
    "1. https://www.geeksforgeeks.org/data-preprocessing-machine-learning-python/?ref=header_search\n",
    "2. https://www.geeksforgeeks.org/ml-feature-scaling-part-2/\n",
    "3. http://localhost:8889/notebooks/tjyana/05-ML/02-Prepare-the-dataset/data-preprocessing-workflow/Preprocessing-Workflow.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a3c10409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "# others: load_diabetes, load_digits, load_boston, load_breast_cancer, load_linnerud, load_sample_image, load_sample_images, load_wine\n",
    "\n",
    "# replacements for boston: \n",
    "\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "# housing = fetch_california_housing()\n",
    "\n",
    "# from sklearn.datasets import fetch_openml\n",
    "# housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "data = fetch_california_housing()\n",
    "\n",
    "df = pd.DataFrame(data = data.data, columns = data.feature_names) # turn to df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69933750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f8643bf",
   "metadata": {},
   "source": [
    "### Duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1fa4ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find duplicates\n",
    "# df.duplicated() returns if duplicated or not \n",
    "\n",
    "# sum it with this\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffa4e4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20635</th>\n",
       "      <td>1.5603</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.045455</td>\n",
       "      <td>1.133333</td>\n",
       "      <td>845.0</td>\n",
       "      <td>2.560606</td>\n",
       "      <td>39.48</td>\n",
       "      <td>-121.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20636</th>\n",
       "      <td>2.5568</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.114035</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>356.0</td>\n",
       "      <td>3.122807</td>\n",
       "      <td>39.49</td>\n",
       "      <td>-121.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20637</th>\n",
       "      <td>1.7000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.205543</td>\n",
       "      <td>1.120092</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>2.325635</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20638</th>\n",
       "      <td>1.8672</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.329513</td>\n",
       "      <td>1.171920</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2.123209</td>\n",
       "      <td>39.43</td>\n",
       "      <td>-121.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20639</th>\n",
       "      <td>2.3886</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.254717</td>\n",
       "      <td>1.162264</td>\n",
       "      <td>1387.0</td>\n",
       "      <td>2.616981</td>\n",
       "      <td>39.37</td>\n",
       "      <td>-121.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20640 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       "\n",
       "       Longitude  \n",
       "0        -122.23  \n",
       "1        -122.22  \n",
       "2        -122.24  \n",
       "3        -122.25  \n",
       "4        -122.25  \n",
       "...          ...  \n",
       "20635    -121.09  \n",
       "20636    -121.21  \n",
       "20637    -121.22  \n",
       "20638    -121.32  \n",
       "20639    -121.24  \n",
       "\n",
       "[20640 rows x 8 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove\n",
    "# df.drop_duplicates()\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24cbfe",
   "metadata": {},
   "source": [
    "### Missing values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382c7933",
   "metadata": {},
   "source": [
    "#### Identifying missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fe7b7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MedInc        0\n",
       "HouseAge      0\n",
       "AveRooms      0\n",
       "AveBedrms     0\n",
       "Population    0\n",
       "AveOccup      0\n",
       "Latitude      0\n",
       "Longitude     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.isnull()\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d852d6",
   "metadata": {},
   "source": [
    "#### Dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2160c3e0",
   "metadata": {},
   "source": [
    "##### remove rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with missing values \n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c38b22",
   "metadata": {},
   "source": [
    "##### drop the column entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90230724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns='COLUMN_NAME', inplace=True) \n",
    "# CHECK THIS ONE BC NOT SURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d63f82",
   "metadata": {},
   "source": [
    "##### imput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imput missing values with mean, median, or mode \n",
    "df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ad8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imput with other data\n",
    "# df['COLUMN_NAME'].replace(np.nan, 'NEW_VALUE', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90d57716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use imputers\n",
    "# SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca058840",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1553292181.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    `df.isnull().sum()`\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "2. Encode categorical variables\n",
    "    - identify `df.select_dtypes(include=['object']).columns`\n",
    "    - choose encoding method\n",
    "        - one-hot\n",
    "            `pd.get_dummies(df, columns=caegorical_cols)`\n",
    "        - label\n",
    "            ```from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    df[col] = label_encoder.fit_transform(df[col])```\n",
    "3. Feature scaling\n",
    "    - import `from sklearn.preprocessing import StandardScaler`\n",
    "    - choose scaling method\n",
    "        - StandardScaler\n",
    "            ```scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)```\n",
    "        - MinMaxScaler\n",
    "            ```from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cfbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a2111d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45b70dda",
   "metadata": {},
   "source": [
    "# HEY LET'S TRY DOING ONE FULL PREPROCESS CYCLE BEFORE TAKING NOTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c00983",
   "metadata": {},
   "source": [
    "# YOU'RE TAKING TOO LONG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb1889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54230172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cef58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c557f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9f64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b37ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = df.fillna(df.mean())\n",
    "train, test = train_test_split(df)\n",
    "train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
